{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katenovita/retail-sales-prediction/blob/main/rossmann_sales_prediction_CNK_vFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rossmann Store Sales - Catherine Novita Kusumaningrum\n",
        "\n",
        "Data derived from Kaggle competition (Florian Knauer and Will Cukierski. Rossmann Store Sales. https://kaggle.com/competitions/rossmann-store-sales, 2015. Kaggle.)"
      ],
      "metadata": {
        "id": "a2mC5igNcjPQ"
      },
      "id": "a2mC5igNcjPQ"
    },
    {
      "cell_type": "markdown",
      "id": "7kOSgEQjaMRG",
      "metadata": {
        "id": "7kOSgEQjaMRG"
      },
      "source": [
        "## Load packages and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B8SXRSO2R23e",
      "metadata": {
        "collapsed": true,
        "id": "B8SXRSO2R23e"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W8yV-xs9nd0x",
      "metadata": {
        "id": "W8yV-xs9nd0x"
      },
      "outputs": [],
      "source": [
        "%pip install optuna optuna-integration[xgboost]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caad38f2",
      "metadata": {
        "id": "caad38f2"
      },
      "outputs": [],
      "source": [
        "# Required packages\n",
        "import os\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "from datetime import timedelta\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Preprocessing and Feature Engineering\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import (\n",
        "    OneHotEncoder,\n",
        "    OrdinalEncoder,\n",
        "    LabelEncoder,\n",
        "    MinMaxScaler,\n",
        "    StandardScaler,\n",
        "    FunctionTransformer,\n",
        ")\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Model Selection and Evaluation\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    cross_val_score,\n",
        "    GridSearchCV,\n",
        "    RandomizedSearchCV,\n",
        "    cross_val_predict,\n",
        "    StratifiedKFold,\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    make_scorer,\n",
        "    log_loss,\n",
        "    roc_curve,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "from optuna.integration import XGBoostPruningCallback\n",
        "import optuna\n",
        "\n",
        "# Machine Learning Models\n",
        "from sklearn.linear_model import Lasso, LogisticRegression\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b7e3e0",
      "metadata": {
        "id": "42b7e3e0"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"rossmann-store-sales.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    for file in zip_ref.namelist():\n",
        "        if file.endswith(\".csv\"):\n",
        "            name = file.replace(\".csv\", \"\").split(\"/\")[-1]\n",
        "            globals()[name] = pd.read_csv(zip_ref.open(file), low_memory=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c866c6b",
      "metadata": {
        "id": "4c866c6b"
      },
      "outputs": [],
      "source": [
        "print(zip_ref.namelist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "247a4c3e",
      "metadata": {
        "id": "247a4c3e"
      },
      "outputs": [],
      "source": [
        "print(\"Train data:\", train.shape)\n",
        "print(train.info())\n",
        "\n",
        "print(\"Test data:\", test.shape)\n",
        "print(test.info())\n",
        "\n",
        "print(\"Store data:\",store.shape)\n",
        "print(store.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d5ceeb",
      "metadata": {
        "id": "58d5ceeb"
      },
      "outputs": [],
      "source": [
        "print(\"Train data: \\n\", train.head())\n",
        "print(\"Test data: \\n\", test.head())\n",
        "print(\"Store data: \\n\", store.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99c59b47",
      "metadata": {
        "id": "99c59b47"
      },
      "outputs": [],
      "source": [
        "# Ensure Date is datetime\n",
        "train['Date'] = pd.to_datetime(train['Date'])\n",
        "test['Date'] = pd.to_datetime(test['Date'])\n",
        "\n",
        "# Breaking down Date into smaller elements\n",
        "train['Year'] = train['Date'].dt.year\n",
        "train['Month'] = train['Date'].dt.month\n",
        "train['Day'] = train['Date'].dt.day\n",
        "train['WeekOfYear'] = train['Date'].dt.isocalendar().week\n",
        "test['Year'] = test['Date'].dt.year\n",
        "test['Month'] = test['Date'].dt.month\n",
        "test['Day'] = test['Date'].dt.day\n",
        "test['WeekOfYear'] = test['Date'].dt.isocalendar().week\n",
        "\n",
        "# fill NAs on 'Open' in test set as 1\n",
        "test['Open'] = test['Open'].fillna(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rWOCUNlyRJhh",
      "metadata": {
        "id": "rWOCUNlyRJhh"
      },
      "outputs": [],
      "source": [
        "# Merge train and test with 'store' df\n",
        "df = train.copy()\n",
        "\n",
        "# Ensure we train only on obs with Open=1 and Sales > 0\n",
        "df = df[df[\"Open\"] != 0]\n",
        "df = df[df[\"Sales\"] > 0]\n",
        "\n",
        "# Standardize Sales into its log form\n",
        "df['log_sales'] = np.log1p(df['Sales'])  # Safe for zero sales\n",
        "\n",
        "# Merge sales and store-related variables\n",
        "df = pd.merge(df, store, on='Store')\n",
        "test = pd.merge(test, store, on='Store')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MpCgrgxZ8Sfk",
      "metadata": {
        "id": "MpCgrgxZ8Sfk"
      },
      "outputs": [],
      "source": [
        "print(df.info())\n",
        "print(test.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19sDZkNgptct",
      "metadata": {
        "id": "19sDZkNgptct"
      },
      "outputs": [],
      "source": [
        "#looping to categorize 'cat' to include categorical columns and 'num' to include numeric columns\n",
        "cat = []\n",
        "num = []\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'O':\n",
        "        cat.append(col)\n",
        "    else:\n",
        "        num.append(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BmQDYd7aDRlI",
      "metadata": {
        "id": "BmQDYd7aDRlI"
      },
      "outputs": [],
      "source": [
        "print(\"Train data:\", df.shape)\n",
        "df[cat].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fagETWoAEBaR",
      "metadata": {
        "id": "fagETWoAEBaR"
      },
      "outputs": [],
      "source": [
        "df[num].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YmSE1Z9vs6q-",
      "metadata": {
        "id": "YmSE1Z9vs6q-"
      },
      "outputs": [],
      "source": [
        "num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VOKHqbWhQ4C9",
      "metadata": {
        "id": "VOKHqbWhQ4C9"
      },
      "outputs": [],
      "source": [
        "cat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aB0XrComQbvy",
      "metadata": {
        "id": "aB0XrComQbvy"
      },
      "source": [
        "#### Visualizing data structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G5PZgQ54px3s",
      "metadata": {
        "id": "G5PZgQ54px3s"
      },
      "outputs": [],
      "source": [
        "# # quick look at the data structure (numerical attributes)\n",
        "plt.figure(figsize=(25,20))\n",
        "for i in range(len(num)):\n",
        "    plt.subplot(8, 3, i+1)\n",
        "    ax = sns.countplot(x=num[i], data=df, legend=False) #, order=df[num[i]].value_counts().index)    # y=num[i],\n",
        "    #plt.bar_label(ax.containers[0], rotation=90, label_type='edge')\n",
        "    sns.despine()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uyI7qji5r16H",
      "metadata": {
        "id": "uyI7qji5r16H"
      },
      "outputs": [],
      "source": [
        "# # quick look at the data structure (categorical attributes)\n",
        "plt.figure(figsize=(30,15))\n",
        "for i in range(len(cat)):\n",
        "    plt.subplot(4, 5, i+1)\n",
        "    ax = sns.countplot(y=cat[i], data=df, legend=False, order=df[cat[i]].value_counts().index)\n",
        "    plt.bar_label(ax.containers[0])\n",
        "    sns.despine()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aRwt4a5K5Vq",
      "metadata": {
        "id": "5aRwt4a5K5Vq"
      },
      "source": [
        "#### Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0jXsXoKYTTs9",
      "metadata": {
        "id": "0jXsXoKYTTs9"
      },
      "outputs": [],
      "source": [
        "df['StateHoliday'] = df['StateHoliday'].replace({'0': '0'})  # fix mixed types\n",
        "df['StateHoliday_flag'] = df['StateHoliday'].ne('0').astype(int)  # 1 if holiday, 0 otherwise\n",
        "test['StateHoliday'] = test['StateHoliday'].replace({'0': '0'})  # fix mixed types\n",
        "test['StateHoliday_flag'] = test['StateHoliday'].ne('0').astype(int)  # 1 if holiday, 0 otherwise\n",
        "\n",
        "correlation = df[['Sales',\n",
        "                  'log_sales',\n",
        "                  'Store',\n",
        "                  'DayOfWeek',\n",
        "                  'Date',\n",
        "#                  'Customers',\n",
        "#                  'Open',\n",
        "                  'Promo',\n",
        "                  'SchoolHoliday',\n",
        "                  'CompetitionDistance',\n",
        "                  'CompetitionOpenSinceMonth',\n",
        "                  'CompetitionOpenSinceYear',\n",
        "                  'Promo2',\n",
        "                  'Promo2SinceWeek',\n",
        "                  'Promo2SinceYear',\n",
        "                  'StateHoliday_flag',\n",
        "                  'Year',\n",
        "                  'Month',\n",
        "                  'Day',\n",
        "                  'WeekOfYear']].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title('Correlation Matrix with Sales')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LggHNEX51aH6",
      "metadata": {
        "id": "LggHNEX51aH6"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D-LjZLRO83zP",
      "metadata": {
        "id": "D-LjZLRO83zP"
      },
      "outputs": [],
      "source": [
        "# Set a seed for reproducibility of this notebook's output\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m8ljUz7T57EQ",
      "metadata": {
        "id": "m8ljUz7T57EQ"
      },
      "outputs": [],
      "source": [
        "# Copy dfs\n",
        "df_copy = df.copy()\n",
        "test_copy = test.copy()\n",
        "\n",
        "# Drop specific columns manually\n",
        "# (Sales are represented now with log_sales, Open has no variation, we don't have # of customers on Test, so we can't predict on Test if training uses Customers)\n",
        "manual_cols = ['Sales','Open','Customers']\n",
        "\n",
        "# Combine and drop columns\n",
        "all_cols_to_drop = manual_cols # list(set(pattern_cols).union(manual_cols))\n",
        "d = df_copy.drop(columns=all_cols_to_drop)\n",
        "test_set = test_copy.drop(columns=['Open']) # We don't train with Open, so this should be excluded for prediction later\n",
        "\n",
        "# Sort by date just to be safe\n",
        "d = d.sort_values('Date')\n",
        "\n",
        "# Set cutoff date (6 weeks, same as per test), to split train set and use for validation\n",
        "cutoff_date = d[\"Date\"].max() - pd.Timedelta(weeks=6)\n",
        "\n",
        "# Time-based split - cutoff of last 6 weeks to mimic the test set\n",
        "train_set = d[d['Date'] <= cutoff_date] # Use strictly for training\n",
        "valid_set = d[d['Date'] > cutoff_date]  # Use to validate training results, before predicting on real test set\n",
        "\n",
        "# Check sample and # of columns per set\n",
        "print(f\"Train shape: {train_set.shape}, Valid shape: {valid_set.shape}, Test shape: {test_set.shape}\")\n",
        "\n",
        "# Remove Date from all dataset\n",
        "train_set = train_set.drop(columns=['Date'])\n",
        "valid_set = valid_set.drop(columns=['Date'])\n",
        "test_set = test_set.drop(columns=['Date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x9-idYaFRMyu",
      "metadata": {
        "id": "x9-idYaFRMyu"
      },
      "outputs": [],
      "source": [
        "train_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DhZFHdE1XWqN",
      "metadata": {
        "id": "DhZFHdE1XWqN"
      },
      "outputs": [],
      "source": [
        "cutoff_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y93d3cgV-XGE",
      "metadata": {
        "id": "Y93d3cgV-XGE"
      },
      "outputs": [],
      "source": [
        "# Keep 'log_sales' as Y column from train, validation sets\n",
        "y_train_log = train_set['log_sales'].copy()\n",
        "y_valid_log = valid_set['log_sales'].copy()\n",
        "\n",
        "# Set back log to its actual sales\n",
        "y_train_sales = np.expm1(y_train_log)\n",
        "y_valid_sales = np.expm1(y_valid_log)\n",
        "\n",
        "# Remove the 'log_sales' column from train, validation sets\n",
        "X_train = train_set.drop(columns=['log_sales'])\n",
        "X_valid = valid_set.drop(columns=['log_sales'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IKGUdCVD9-ah",
      "metadata": {
        "id": "IKGUdCVD9-ah"
      },
      "outputs": [],
      "source": [
        "### Define functions to build preprocessing pipeline\n",
        "\n",
        "## Passthrough Transformer to create placeholder step for features that don't need to be transformed\n",
        "class PassthroughTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X\n",
        "\n",
        "    def get_feature_names_out(self, input_features=None):\n",
        "        return input_features\n",
        "\n",
        "# Define the PassthroughTransformer\n",
        "passthrough_transformer = PassthroughTransformer()\n",
        "\n",
        "# to scale & impute NAs in 'CompetitionDistance' - it doesn't make sense if distance is 0, so it must be assumed to be very high number/far away from each other\n",
        "num_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy=\"constant\", fill_value=99999)),\n",
        "    ('standard', StandardScaler())\n",
        "])\n",
        "\n",
        "# MinMix scaling for the rest of numerical vars that we know have certain bounds\n",
        "num_minmax_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy=\"constant\", fill_value=0)), #fillnas as 0\n",
        "    ('minmax', MinMaxScaler())\n",
        "])\n",
        "\n",
        "# # Define a custom transformer to convert data to string\n",
        "class ToStringTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Ensure X is treated as a pandas DataFrame for .astype(str)\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "             # Handle case where X might be a numpy array from ColumnTransformer\n",
        "             # Assuming single column input based on usage\n",
        "             X = pd.DataFrame(X, columns=self.feature_names_in_)\n",
        "\n",
        "        return X.astype(str)\n",
        "\n",
        "    def get_feature_names_out(self, input_features=None):\n",
        "        # This method is required for compatibility with ColumnTransformer\n",
        "        # It should return the input feature names as the output feature names\n",
        "        return input_features if input_features is not None else self.feature_names_in_\n",
        "\n",
        "# Build pipeline to correctly encode Store\n",
        "store_transformer = Pipeline(steps=[\n",
        "    ('to_string', ToStringTransformer()),\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='0')),\n",
        "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "])\n",
        "\n",
        "# For PromoInterval, one-hot encode (NAs will also be encoded as 0)\n",
        "cat_promoint_pipeline = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "cat_ohe_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o2L7zMMS-Oxy",
      "metadata": {
        "id": "o2L7zMMS-Oxy"
      },
      "outputs": [],
      "source": [
        "# Features to transform\n",
        "num_features = ['CompetitionDistance']\n",
        "minmax_features = ['CompetitionOpenSinceMonth','CompetitionOpenSinceYear','Promo2SinceWeek','Promo2SinceYear','Year','Month','Day','WeekOfYear']\n",
        "binary_features = ['Promo','Promo2','SchoolHoliday']\n",
        "store_features = ['Store']\n",
        "promo_features = ['PromoInterval']\n",
        "\n",
        "# Filter out the ordinal ones, to one-hot encode categorical features that has no \"levels\"\n",
        "ohe_features = ['StoreType','Assortment','DayOfWeek','StateHoliday']  #DoW better be ohe https://otexts.com/fpp2/useful-predictors.html\n",
        "\n",
        "# ColumnTransformer to tie all together\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', num_pipeline, num_features),\n",
        "    ('minmax', num_minmax_pipeline, minmax_features),\n",
        "    ('binary', PassthroughTransformer(), binary_features),\n",
        "    ('store', store_transformer, store_features),\n",
        "    ('promo', cat_promoint_pipeline, promo_features),\n",
        "    ('ohe', cat_ohe_pipeline, ohe_features),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ntzVzEZsBewG",
      "metadata": {
        "id": "ntzVzEZsBewG"
      },
      "outputs": [],
      "source": [
        "# Fit and transform the pipeline only on the training set\n",
        "train_prepared = preprocessor.fit_transform(X_train)\n",
        "print(train_prepared)\n",
        "\n",
        "# Transform pipeline on valid and test sets\n",
        "valid_prepared = preprocessor.transform(X_valid)\n",
        "test_prepared = preprocessor.transform(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QbS7P3QYHUmr",
      "metadata": {
        "id": "QbS7P3QYHUmr"
      },
      "outputs": [],
      "source": [
        "# Turn numpy output into pd.DataFrame\n",
        "column_names = preprocessor.get_feature_names_out()\n",
        "X_train_prepared = pd.DataFrame(train_prepared, columns=column_names)\n",
        "X_valid_prepared = pd.DataFrame(valid_prepared, columns=column_names)\n",
        "test_prepared_df = pd.DataFrame(test_prepared, columns=column_names)\n",
        "\n",
        "# Now you can use .head()\n",
        "X_train_prepared.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uX67bIB8ohbu",
      "metadata": {
        "id": "uX67bIB8ohbu"
      },
      "outputs": [],
      "source": [
        "X_train_prepared.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4spsQOoHZcWC",
      "metadata": {
        "id": "4spsQOoHZcWC"
      },
      "outputs": [],
      "source": [
        "# Sanitize problematic chars in column names\n",
        "X_train_prepared.columns = X_train_prepared.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
        "X_valid_prepared.columns = X_valid_prepared.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
        "test_prepared_df.columns = test_prepared_df.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZAStH41MxfrZ",
      "metadata": {
        "id": "ZAStH41MxfrZ"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DmE4wJHZjay_",
      "metadata": {
        "id": "DmE4wJHZjay_"
      },
      "outputs": [],
      "source": [
        "# Define global functions for evaluating the models based on RMSPE and scaling back to actual sales metrics\n",
        "\n",
        "def rmspe(y_true, y_pred):\n",
        "    \"\"\"Root Mean Squared Percentage Error.\"\"\"\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "def evaluate_model(y_valid_log, y_pred_log, model_name=\"Model\"):\n",
        "    \"\"\"Evaluate both log-scale and original sales scale metrics.\"\"\"\n",
        "\n",
        "    # RMSE in log space\n",
        "    rmse_log = np.sqrt(mean_squared_error(y_valid_log, y_pred_log))\n",
        "    rmspe_log = rmspe(y_valid_log, y_pred_log)\n",
        "\n",
        "    # Convert back to original sales values\n",
        "    y_valid_sales = np.expm1(y_valid_log)\n",
        "    y_pred_sales = np.expm1(y_pred_log)\n",
        "\n",
        "    # RMSE and RMSPE on real sales values\n",
        "    rmse_sales = np.sqrt(mean_squared_error(y_valid_sales, y_pred_sales))\n",
        "    rmspe_sales = rmspe(y_valid_sales, y_pred_sales)\n",
        "\n",
        "    print(f\"✅ {model_name} Evaluation:\")\n",
        "    print(f\"  • RMSE (log): {rmse_log:.4f}\")\n",
        "    print(f\"  • RMSPE (log): {rmspe_log:.4f}\")\n",
        "    print(f\"  • RMSE (sales): {rmse_sales:.4f}\")\n",
        "    print(f\"  • RMSPE (sales): {rmspe_sales:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    return {\n",
        "        \"rmse_log\": rmse_log,\n",
        "        \"rmspe_log\": rmspe_log,\n",
        "        \"rmse_sales\": rmse_sales,\n",
        "        \"rmspe_sales\": rmspe_sales\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "px05dO4dyNo-",
      "metadata": {
        "id": "px05dO4dyNo-"
      },
      "source": [
        "### Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o_QtKOQrCuKN",
      "metadata": {
        "id": "o_QtKOQrCuKN"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(X_train_prepared, y_train_log)\n",
        "y_pred_log = reg.predict(X_valid_prepared)\n",
        "# Evaluate\n",
        "evaluate_model(y_valid_log, y_pred_log, model_name=\"Linear Regression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kMMEysGtx8D6",
      "metadata": {
        "id": "kMMEysGtx8D6"
      },
      "source": [
        "### LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SKUT9QNYd00z",
      "metadata": {
        "id": "SKUT9QNYd00z"
      },
      "outputs": [],
      "source": [
        "# 1. LightGBM default\n",
        "lgb_model = LGBMRegressor()\n",
        "lgb_model.fit(X_train_prepared, y_train_log)\n",
        "y_pred_log = lgb_model.predict(X_valid_prepared)\n",
        "\n",
        "# Evaluate\n",
        "evaluate_model(y_valid_log, y_pred_log, model_name=\"LightGBM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LightGBM hyperparameter tuning"
      ],
      "metadata": {
        "id": "ZIDMXOA-aGRp"
      },
      "id": "ZIDMXOA-aGRp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dB1d7oLMs5i2",
      "metadata": {
        "id": "dB1d7oLMs5i2"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10.0, log=True),\n",
        "        'random_state': 42\n",
        "    }\n",
        "\n",
        "    model = LGBMRegressor(**params)\n",
        "    model.fit(\n",
        "        X_train_prepared, y_train_log,\n",
        "        eval_set=[(X_valid_prepared, y_valid_log)],\n",
        "        eval_metric=\"rmse\"\n",
        "    )\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_log = model.predict(X_valid_prepared)\n",
        "    y_pred_sales = np.expm1(y_pred_log)\n",
        "    y_valid_sales = np.expm1(y_valid_log)\n",
        "\n",
        "    # Calculate RMSPE (sales)\n",
        "    rmspe_sales = rmspe(y_valid_sales, y_pred_sales)\n",
        "\n",
        "    return rmspe_sales  # must return a single float\n",
        "\n",
        "lgb_study = optuna.create_study(study_name=\"rossmann_lgbm_study\", direction='minimize')\n",
        "lgb_study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
        "\n",
        "if lgb_study.best_trial:\n",
        "    print(\"Best RMSE:\", lgb_study.best_value)\n",
        "    print(\"Best Params:\", lgb_study.best_trial.params)\n",
        "else:\n",
        "    print(\"No successful trials.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QqOj7vyOARIg",
      "metadata": {
        "id": "QqOj7vyOARIg"
      },
      "outputs": [],
      "source": [
        "best_params_lgb = lgb_study.best_params\n",
        "best_params_lgb.update({'objective': 'regression', 'metric': 'rmse'})\n",
        "best_model_lgb = lgb.train(best_params_lgb, lgb.Dataset(X_train_prepared, label=y_train_log),\n",
        "                  valid_sets=[lgb.Dataset(X_valid_prepared, label=y_valid_log)],\n",
        "                  )\n",
        "\n",
        "# Predict (in log scale), then convert to real sales\n",
        "y_pred_log = best_model_lgb.predict(X_valid_prepared)\n",
        "y_pred_sales = np.expm1(y_pred_log)\n",
        "y_valid_sales = np.expm1(y_valid_log)\n",
        "\n",
        "# Evaluate\n",
        "evaluate_model(y_valid_log, y_pred_log, model_name=\"Best Tuned LightGBM\")\n",
        "\n",
        "# Plot feature importance\n",
        "fig, ax = plt.subplots(figsize=(10, 8))  # adjust size as needed\n",
        "lgb.plot_importance(best_model_lgb, max_num_features=20, importance_type='gain', height=0.5, ax=ax)\n",
        "\n",
        "ax.grid(False)  # Remove grid lines\n",
        "ax.set_title(\"Top 20 Feature Importances LightGBM\", fontsize=14)\n",
        "plt.tight_layout(pad=1.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VJsK_wmWyWt5",
      "metadata": {
        "id": "VJsK_wmWyWt5"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "evWT0aWBcLx2",
      "metadata": {
        "id": "evWT0aWBcLx2"
      },
      "outputs": [],
      "source": [
        "# Create DMatrix\n",
        "dtrain = xgb.DMatrix(X_train_prepared, label=y_train_log)\n",
        "dvalid = xgb.DMatrix(X_valid_prepared, label=y_valid_log)\n",
        "\n",
        "# Set parameters with GPU support\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'eval_metric': 'rmse',\n",
        "    'tree_method': \"hist\",\n",
        "    'device': \"cuda\",\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train model\n",
        "evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
        "model = xgb.train(params, dtrain, num_boost_round=1000, evals=evals,\n",
        "                  early_stopping_rounds=50, verbose_eval=100)\n",
        "\n",
        "# Predict (in log scale), then convert to real sales\n",
        "y_pred_log = model.predict(dvalid)\n",
        "\n",
        "# Evaluate\n",
        "evaluate_model(y_valid_log, y_pred_log, model_name=\"XGBoost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dywkHp5AzlFY",
      "metadata": {
        "id": "dywkHp5AzlFY"
      },
      "source": [
        "#### XGBoost hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mLWMvHzY0K6x",
      "metadata": {
        "id": "mLWMvHzY0K6x"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # Suggest hyperparameters space to search\n",
        "    param = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'eval_metric': 'rmse',\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'lambda': trial.suggest_float('lambda', 1e-4, 10.0, log=True),\n",
        "        'alpha': trial.suggest_float('alpha', 1e-4, 10.0, log=True),\n",
        "        'tree_method': 'hist',\n",
        "        'device': \"cuda\",\n",
        "        'seed': 42\n",
        "    }\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_train_prepared, label=y_train_log)\n",
        "    dvalid = xgb.DMatrix(X_valid_prepared, label=y_valid_log)\n",
        "\n",
        "    evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
        "\n",
        "    model = xgb.train(\n",
        "        params=param,\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=1000,\n",
        "        evals=evals,\n",
        "        early_stopping_rounds=50,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_log = model.predict(dvalid)\n",
        "    y_pred_sales = np.expm1(y_pred_log)\n",
        "    y_valid_sales = np.expm1(y_valid_log)\n",
        "\n",
        "    # Calculate RMSPE (sales)\n",
        "    rmspe_sales = rmspe(y_valid_sales, y_pred_sales)\n",
        "\n",
        "    return rmspe_sales  # must return a single float\n",
        "\n",
        "# Create and run the optimization process with 30 trials\n",
        "study_xgb = optuna.create_study(study_name=\"rossmann_xgboost_study\", direction='minimize')\n",
        "study_xgb.optimize(objective, n_trials=30, show_progress_bar=True, n_jobs=-1)\n",
        "\n",
        "# Retrieve the best parameter values\n",
        "best_params_xgb = study_xgb.best_params\n",
        "print(f\"\\nBest parameters: {best_params_xgb}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xXHvzF9IhENi",
      "metadata": {
        "id": "xXHvzF9IhENi"
      },
      "outputs": [],
      "source": [
        "# Retrieve the best parameter values\n",
        "best_params_xgb = study_xgb.best_params\n",
        "print(f\"\\nBest parameters: {best_params_xgb}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8jc-MLzK2Pj",
      "metadata": {
        "id": "f8jc-MLzK2Pj"
      },
      "outputs": [],
      "source": [
        "# Apply the best params on the valid/test set\n",
        "best_model_xgb = xgb.train(\n",
        "    params=best_params_xgb,\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=1000,\n",
        "    evals=evals,\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=100\n",
        ")\n",
        "\n",
        "# Predict (in log scale), then convert to real sales\n",
        "y_pred_log = best_model_xgb.predict(dvalid)\n",
        "y_pred_sales = np.expm1(y_pred_log)\n",
        "y_valid_sales = np.expm1(y_valid_log)\n",
        "\n",
        "# Evaluate\n",
        "evaluate_model(y_valid_log, y_pred_log, model_name=\"Best Tuned XGBoost\")\n",
        "\n",
        "# Plot feature importance\n",
        "fig, ax = plt.subplots(figsize=(10, 10))  # adjust size as needed\n",
        "xgb.plot_importance(best_model_xgb, max_num_features=20, importance_type='gain', height=0.5, ax=ax)\n",
        "\n",
        "ax.grid(False)  # Remove grid lines\n",
        "ax.set_title(\"Top 20 Feature Importances XGBoost\", fontsize=14)\n",
        "plt.tight_layout(pad=1.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Results\n",
        "models = ['Linear Regression', 'LightGBM', 'XGBoost']\n",
        "rmse_sales = [2603.3063, 1322.0624, 885.7547]\n",
        "rmspe_sales = [0.4325, 0.1829, 0.1240]\n",
        "\n",
        "# Create figure with two subplots side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# --- RMSE (Sales) ---\n",
        "bars1 = axes[0].bar(models, rmse_sales, color=['#A5C9CA', '#E7F6F2', '#395B64'])\n",
        "axes[0].set_title('RMSE (Sales) Comparison')\n",
        "axes[0].set_ylabel('RMSE')\n",
        "axes[0].set_xlabel('Model')\n",
        "\n",
        "# Add value annotations\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, height,\n",
        "                 f\"{height:,.0f}\", ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# --- RMSPE (Sales) ---\n",
        "bars2 = axes[1].bar(models, rmspe_sales, color=['#A5C9CA', '#E7F6F2', '#395B64'])\n",
        "axes[1].set_title('RMSPE (Sales) Comparison')\n",
        "axes[1].set_ylabel('RMSPE')\n",
        "axes[1].set_xlabel('Model')\n",
        "\n",
        "# Add value annotations\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2, height,\n",
        "                 f\"{height:.3f}\", ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.suptitle('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R37G2e2KkLUX"
      },
      "id": "R37G2e2KkLUX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ur46vB1VxuCn",
      "metadata": {
        "id": "ur46vB1VxuCn"
      },
      "source": [
        "#### Save the best model and re-use it on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5VpZ_z5wLliV",
      "metadata": {
        "id": "5VpZ_z5wLliV"
      },
      "outputs": [],
      "source": [
        "best_model_xgb.save_model(\"best_xgb_model.pkl\")\n",
        "# loaded_model = xgb.Booster()\n",
        "# loaded_model.load_model(\"best_xgb_model.pkl\")\n",
        "\n",
        "# # Predict (in log scale), then convert to real sales\n",
        "# y_pred_log = loaded_model.predict(dvalid)\n",
        "\n",
        "# # Evaluate\n",
        "# evaluate_model(y_valid_log, y_pred_log, model_name=\"Best Tuned XGBoost\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QQ9RlLmR2K3G",
      "metadata": {
        "id": "QQ9RlLmR2K3G"
      },
      "outputs": [],
      "source": [
        "# Convert to DMatrix\n",
        "dtest = xgb.DMatrix(test_prepared_df)\n",
        "\n",
        "# Predict on actual test set\n",
        "y_test_log_pred = best_model_xgb.predict(dtest)\n",
        "y_test_pred = np.expm1(y_test_log_pred)  # Convert back from log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CurHurtj2P_i",
      "metadata": {
        "id": "CurHurtj2P_i"
      },
      "outputs": [],
      "source": [
        "# Final output - consist of 'Id' and predicted 'Sales'\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test['Id'],\n",
        "    'Open': test['Open'],\n",
        "    'Sales': y_test_pred\n",
        "})\n",
        "\n",
        "# Ensure that closed obs has 0 Sales even on prediction and no negative predictions (competition rule)\n",
        "submission['Sales'] = submission['Sales'].clip(lower=0)\n",
        "submission.loc[test['Open'] == 0, 'Sales'] = 0\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test['Id'],\n",
        "    'Sales': y_test_pred\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8vPnINJfu7OR",
      "metadata": {
        "id": "8vPnINJfu7OR"
      },
      "outputs": [],
      "source": [
        "submission.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pNQriHrBD-sg",
      "metadata": {
        "id": "pNQriHrBD-sg"
      },
      "source": [
        "#### Saving final model and submission on my GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tmoohWZwDPOo",
      "metadata": {
        "id": "tmoohWZwDPOo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qYeI_HG8D-DL",
      "metadata": {
        "id": "qYeI_HG8D-DL"
      },
      "outputs": [],
      "source": [
        "# Set your Drive folder\n",
        "target_folder = '/content/drive/My Drive/Colab Notebooks/0_Portfolio/outputs'\n",
        "os.makedirs(target_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qbCIwhk5EOnt",
      "metadata": {
        "id": "qbCIwhk5EOnt"
      },
      "outputs": [],
      "source": [
        "import joblib  # or pickle\n",
        "\n",
        "# Example DataFrame and model\n",
        "submission_df = submission\n",
        "best_model = best_model_xgb\n",
        "\n",
        "# Save CSV\n",
        "csv_path = os.path.join(target_folder, 'submission.csv')\n",
        "submission_df.to_csv(csv_path, index=False)\n",
        "\n",
        "# Save model as .pkl\n",
        "model_path = os.path.join(target_folder, 'best_model_xgb.pkl')\n",
        "joblib.dump(best_model, model_path)\n",
        "\n",
        "print(f\"Saved CSV to {csv_path}\")\n",
        "print(f\"Saved model to {model_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
